{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear versus Ridge Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Linear regression was one of the earliest types of regression that studied rigorously. It is widely used for predicting continuous values. For that reason, we used the “Boston house prices dataset” to address this regression problem. The model takes into account 13 attributes to try to predict the price of a house in Boston.\n",
    "\n",
    "The program itself should be run linearly as to ensure the appropriate libraries are loaded and all data pre-processing occurs before we run any regressions. We can then run the regressions and see the performance of different regression parameters in the 11th and 12th code block. By observing the train and test error, we can determine that the best performing model is a Linear Regression with Polynomial Features of Second Order. Since it contains 105 parameters, it would be impractical for me to list them in this report, but they can be found in the 13th code block of the program. We can see that the parameters vary across many degrees of magnitude all the way from e-05 to e+01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:  Getting, understanding, and preprocessing the dataset\n",
    "\n",
    "We first import the standard libaries and some libraries that will help us scale the data and perform some \"feature engineering\" by transforming the data into $\\Phi_2({\\bf x})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import matmul\n",
    "import sklearn\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the boston dataset from sklearn\n",
    "# Load dataset to some variable \n",
    "boston_data = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of features is:  13\n",
      "The features:  ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "The number of exampels in our dataset:  506\n",
      "[[6.3200e-03 1.8000e+01 2.3100e+00 0.0000e+00 5.3800e-01 6.5750e+00\n",
      "  6.5200e+01 4.0900e+00 1.0000e+00 2.9600e+02 1.5300e+01 3.9690e+02\n",
      "  4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 6.4210e+00\n",
      "  7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9690e+02\n",
      "  9.1400e+00]]\n"
     ]
    }
   ],
   "source": [
    "#  Create X and Y variables - X holding the .data and Y holding .target \n",
    "X = boston_data.data\n",
    "y = boston_data.target\n",
    "\n",
    "\n",
    "#  Reshape Y to be a rank 2 matrix using y.reshape()\n",
    "# y = y.reshape(int(y.size/2),2)\n",
    "# print(\"y has been reshaped to \" + str(y.shape))\n",
    "\n",
    "# Observe the number of features and the number of labels\n",
    "print('The number of features is: ', X.shape[1])\n",
    "# Printing out the features\n",
    "print('The features: ', boston_data.feature_names)\n",
    "# The number of examples\n",
    "print('The number of exampels in our dataset: ', X.shape[0])\n",
    "# Observing the first 2 rows of the data\n",
    "print(X[0:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also create polynomial feeatures for the dataset to test linear and ridge regression on data with d = 1 and data with d = 2. Feel free to increase the # of degress and see what effect it has on the training and test error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PolynomialFeatures object with degree = 2. Using PolynomialFeatures(degree=2)\n",
    "# Transform X and save it into X_2 using poly.fit_transform(X)\n",
    "# Simply copy Y into Y_2 \n",
    "poly = PolynomialFeatures(2)\n",
    "X_2 = poly.fit_transform(X)\n",
    "y_2 = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 105)\n",
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "# the shape of X_2 and Y_2 - should be (506, 105) and (506, 1) respectively\n",
    "print(X_2.shape)\n",
    "print(y_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the get_coeff_ridge_normaleq function. Use the normal equation method.\n",
    "# Return w values\n",
    "\n",
    "def get_coeff_ridge_normaleq(X_train, y_train, alpha):\n",
    "    # use np.linalg.pinv(...)\n",
    "    X_T = np.transpose(X_train)\n",
    "    w = matmul(matmul(np.linalg.pinv(matmul(X_T,X_train)+(alpha*np.identity(np.size(X_train,1)))),X_T),y_train)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the get_coeff_ridge_normaleq function. Use the normal equation method.\n",
    "# Return w values\n",
    "\n",
    "def get_coeff_linear_normaleq(X_train, y_train):\n",
    "    # use np.linalg.pinv(...)\n",
    "    X_T = np.transpose(X_train)\n",
    "    w = matmul(matmul(np.linalg.pinv(matmul(X_T,X_train)),X_T),y_train)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluate_err_ridge function.\n",
    "# Return the train_error and test_error values\n",
    "\n",
    "\n",
    "def evaluate_err(X_train, X_test, y_train, y_test, w): \n",
    "#     pred_train=prediction using w and X_train+np.mean(y_train)\n",
    "    pred_train = matmul(w,np.transpose(X_train))\n",
    "#     pred_test=prediction using w and X_test\n",
    "    pred_test = matmul(w,np.transpose(X_test))\n",
    "#     remember to add the mean back\n",
    "\n",
    "    train_error= np.sum(np.square(pred_train - y_train))/int(y.size)\n",
    "    test_error= np.sum(np.square(pred_test - y_test))/int(y.size)\n",
    "    \n",
    "    return train_error, test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish writting the k_fold_cross_validation function. \n",
    "# Returns the average training error and average test error from the k-fold cross validation\n",
    "# Sklearns K-Folds cross-validator: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "\n",
    "def k_fold_cross_validation(k, X, y, alpha=None):\n",
    "    kf = KFold(n_splits=k, random_state=21, shuffle=True)\n",
    "    total_E_val_test = 0\n",
    "    total_E_val_train = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Centering the data so we do not need the intercept term (we could have also chose w_0=average y value)\n",
    "        # Subtract y_train_mean from y_train and y_test\n",
    "        y_train_mean = np.average(y_train)\n",
    "        y_train = y_train - y_train_mean\n",
    "        y_test = y_test - y_train_mean\n",
    "        \n",
    "        # Scaling the data matrix\n",
    "        # Using scaler=preprocessing.StandardScaler().fit(...)\n",
    "        # And scaler.transform(...)\n",
    "        scaler=preprocessing.StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        # Determine the training error and the test error\n",
    "        # Use get_coeff_linear_normaleq or get_coeff_ridge_normaleq to get w\n",
    "        # And use evaluate_err()\n",
    "        if alpha == None:\n",
    "            w = get_coeff_linear_normaleq(X_train, y_train)\n",
    "        else:\n",
    "            w = get_coeff_ridge_normaleq(X_train, y_train, alpha)\n",
    "        train_error, test_error = evaluate_err(X_train, X_test, y_train, y_test, w)\n",
    "        total_E_val_test = total_E_val_test + test_error\n",
    "        total_E_val_train = total_E_val_train +train_error\n",
    "        \n",
    "\n",
    "\n",
    "       ##############\n",
    "    return  total_E_val_test, total_E_val_train\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_error(alpha, total_E_val_test, total_E_val_train):\n",
    "    print(\"----------{:.6}----------\".format(alpha))\n",
    "    print(\"Test Error: {} Train Error: {}\".format(total_E_val_test, total_E_val_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Linear----------\n",
      "Test Error: 23.814099478324252 Train Error: 20.903355270040077\n",
      "----------10.0----------\n",
      "Test Error: 23.831212525393095 Train Error: 20.978438681623143\n",
      "----------31.6228----------\n",
      "Test Error: 24.0967636639064 Train Error: 21.326253537929905\n",
      "----------100.0----------\n",
      "Test Error: 25.231522741511704 Train Error: 22.619174073116447\n",
      "----------316.228----------\n",
      "Test Error: 29.093670513595224 Train Error: 26.634588774931718\n",
      "----------1000.0----------\n",
      "Test Error: 38.77449026538695 Train Error: 36.21614096894125\n",
      "----------3162.28----------\n",
      "Test Error: 53.80828954499053 Train Error: 50.845324814074424\n",
      "----------10000.0----------\n",
      "Test Error: 69.10554389305757 Train Error: 65.61090492154696\n",
      "----------31622.8----------\n",
      "Test Error: 78.67160621565678 Train Error: 74.81275173599427\n",
      "----------1e+05----------\n",
      "Test Error: 82.73042532923382 Train Error: 78.71260919918016\n",
      "----------3.16228e+05----------\n",
      "Test Error: 84.15615474914756 Train Error: 80.08201723413298\n",
      "----------1e+06----------\n",
      "Test Error: 84.62291415365209 Train Error: 80.5302885101912\n",
      "----------3.16228e+06----------\n",
      "Test Error: 84.77216622306743 Train Error: 80.67362379262192\n",
      "----------1e+07----------\n",
      "Test Error: 84.81953079611014 Train Error: 80.71911019692958\n"
     ]
    }
   ],
   "source": [
    "# print the error for the both linear regression and ridge regression\n",
    "# the error should include both training error and testing error\n",
    "\n",
    "k = 23\n",
    "\n",
    "# Error for linear regression\n",
    "total_E_val_test, total_E_val_train = k_fold_cross_validation(k, X, y)\n",
    "print_error(\"Linear\", total_E_val_test, total_E_val_train/k)\n",
    "\n",
    "# Error for Ridge Regression\n",
    "for alpha in np.logspace(1,7, num=13):\n",
    "    total_E_val_test, total_E_val_train = k_fold_cross_validation(k, X, y, alpha)\n",
    "    print_error(alpha, total_E_val_test, total_E_val_train/k)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Linear----------\n",
      "Test Error: 12.32424061557486 Train Error: 5.65499187495009\n",
      "----------10.0----------\n",
      "Test Error: 13.387672506197465 Train Error: 9.597239786830333\n",
      "----------31.6228----------\n",
      "Test Error: 15.764457061218113 Train Error: 12.11395337179339\n",
      "----------100.0----------\n",
      "Test Error: 18.957402274368985 Train Error: 15.419017855937915\n",
      "----------316.228----------\n",
      "Test Error: 22.01267239266144 Train Error: 18.730018758978243\n",
      "----------1000.0----------\n",
      "Test Error: 25.942627911879896 Train Error: 22.98981030792135\n",
      "----------3162.28----------\n",
      "Test Error: 33.94987970614068 Train Error: 31.118480179006053\n",
      "----------10000.0----------\n",
      "Test Error: 47.012890618203926 Train Error: 44.012304097886734\n",
      "----------31622.8----------\n",
      "Test Error: 61.93124955396309 Train Error: 58.5949765844866\n",
      "----------1e+05----------\n",
      "Test Error: 74.31660686918063 Train Error: 70.5949619283571\n",
      "----------3.16228e+05----------\n",
      "Test Error: 80.9455591312574 Train Error: 76.98903717112036\n",
      "----------1e+06----------\n",
      "Test Error: 83.5391056233602 Train Error: 79.48669495631951\n",
      "----------3.16228e+06----------\n",
      "Test Error: 84.42204716370539 Train Error: 80.33654885630538\n",
      "----------1e+07----------\n",
      "Test Error: 84.7080562699166 Train Error: 80.61179413731003\n"
     ]
    }
   ],
   "source": [
    "# Model with Polynomial Features\n",
    "\n",
    "k=23\n",
    "\n",
    "# Error for linear regression\n",
    "total_E_val_test, total_E_val_train = k_fold_cross_validation(k, X_2, y_2)\n",
    "print_error(\"Linear\", total_E_val_test, total_E_val_train/k)\n",
    "\n",
    "# Error for Ridge Regression\n",
    "for alpha in np.logspace(1,7, num=13):\n",
    "    total_E_val_test, total_E_val_train = k_fold_cross_validation(k, X_2, y_2, alpha)\n",
    "    print_error(alpha, total_E_val_test, total_E_val_train/k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.89176201e-01 -6.96195792e+00  2.64067984e-01 -3.10232186e+00\n",
      " -2.61115550e-01 -7.98648586e-02  1.19457489e+01  7.42708817e-01\n",
      " -1.52258329e+01  3.31403243e+00 -1.25220413e-01  1.32411984e-01\n",
      "  3.17545781e-02  8.91696273e-02  1.45253857e-03  1.67053032e-01\n",
      "  2.22052033e-01  2.81094561e+00 -3.63747353e-01  1.56108868e-01\n",
      " -2.80410199e-03 -1.06722114e-01 -5.95086552e-02 -3.40385362e-03\n",
      "  2.85411313e-01 -3.49399266e-04  2.12283479e-02 -2.88370318e-04\n",
      " -4.72323816e-03 -4.66579035e-02 -1.14311965e+00  5.70934279e-03\n",
      "  4.37232955e-05 -1.18174261e-02 -3.74857158e-03  5.40368258e-04\n",
      " -7.16740379e-03  7.88659907e-04 -4.51780529e-03  2.78758895e-02\n",
      " -1.87410822e-01  1.04644822e+00  1.49957416e-01  4.30453942e-03\n",
      "  8.77936473e-02 -6.42096125e-02  8.54014583e-04 -2.61454187e-02\n",
      "  2.89148389e-03 -1.51426896e-02 -2.61095864e-01 -1.60571716e+00\n",
      " -4.36017523e+00  8.16306787e-02  1.81008721e+00 -5.76672238e-01\n",
      " -1.26071180e-03  7.25845292e-01  3.98585840e-02 -4.56952828e-01\n",
      " -7.68771253e-01  7.06943573e+00 -4.27248197e-01  1.83998081e+01\n",
      " -2.45052295e+00  1.73849642e-01 -8.33077858e+00 -2.17520223e-02\n",
      "  1.44233668e+00  3.87629849e-01 -3.95152438e-02  3.77419284e-01\n",
      " -2.24362720e-01 -9.07619637e-03 -3.61221914e-01 -4.08196024e-03\n",
      " -1.98513035e-01  9.75548028e-05 -4.80813108e-03  1.60899812e-02\n",
      " -5.88623046e-04  6.94212044e-04 -5.86859703e-04 -6.41639324e-03\n",
      "  4.79537653e-01 -1.48790301e-01 -2.93976928e-03 -3.90124256e-02\n",
      " -1.37119571e-03  1.06863628e-01 -1.19435638e-01  7.88633133e-03\n",
      " -3.65583684e-02 -7.32553805e-04 -3.84994398e-02 -7.17838477e-05\n",
      "  7.96776867e-03 -3.76982824e-05 -2.28778040e-04  5.84547752e-02\n",
      "  4.18090947e-03  2.03944441e-02 -3.70121496e-05 -4.05143603e-04\n",
      "  1.33190588e-02]\n"
     ]
    }
   ],
   "source": [
    "# Getting parameters of the best model [Linear Regression with Polynomial Features of second order]\n",
    "\n",
    "w = get_coeff_linear_normaleq(X_2, y_2)\n",
    "print(w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
